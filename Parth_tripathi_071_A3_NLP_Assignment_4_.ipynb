{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parthsantosh2411/Hindi-Summarization/blob/main/Parth_tripathi_071_A3_NLP_Assignment_4_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDWnbAvoAgT6"
      },
      "source": [
        "`Name:` **Parth Santosh Tripathi**  \n",
        "\n",
        "`PRN:` **22070126071**  \n",
        "\n",
        "`Class:` **AIML A3**  \n",
        "\n",
        "`Assignment No:` **4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6aV3jxDAgT8"
      },
      "source": [
        "### Import Libraries and Set Seed\n",
        "This cell imports the necessary libraries, including PyTorch for deep learning and NLTK for tokenizing text. It also sets a random seed to ensure reproducibility of results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WuxAHxGY_6C",
        "outputId": "3974f196-2503-4ea5-c1d1-5e3fb83313ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import Necessary Libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from rouge import Rouge\n",
        "import os\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57DdPMhgAgT8"
      },
      "source": [
        "### BiLSTM Model for Text Summarization\n",
        "\n",
        "The `BiLSTMSummarizer` class defines a Bidirectional LSTM-based model for text summarization. Here's a detailed breakdown of its components:\n",
        "\n",
        "1. **Embedding Layer**:\n",
        "    - The first layer in the model is an embedding layer that converts input words (represented as indices) into dense vectors (embeddings). The embedding size is defined by `embedding_dim`.\n",
        "\n",
        "2. **Bidirectional LSTM Encoder**:\n",
        "    - The encoder is a bidirectional LSTM (`nn.LSTM`) that processes the input sequence in both forward and backward directions, helping capture context from both sides of the sentence. The hidden size is defined by `hidden_dim`.\n",
        "\n",
        "3. **Decoder LSTM**:\n",
        "    - The decoder is an LSTM that takes the concatenated hidden states from both directions of the encoder as input. It generates the target sequence step-by-step, using either teacher forcing (actual next token) or predicted tokens.\n",
        "\n",
        "4. **Fully Connected Layer**:\n",
        "    - After passing through the decoder, the output is fed into a fully connected layer that maps the hidden state to the vocabulary size, resulting in a probability distribution over the vocabulary for the next word prediction.\n",
        "\n",
        "#### Forward Pass:\n",
        "\n",
        "- **Input**:\n",
        "    - `src`: Source sequence (input article).\n",
        "    - `trg`: Target sequence (summary).\n",
        "    - `teacher_forcing_ratio`: A hyperparameter that controls the use of teacher forcing, where the correct output token is fed back into the model at each time step (as opposed to the model’s predicted token).\n",
        "    \n",
        "- **Process**:\n",
        "    - The input sequence `src` is first embedded into dense vectors.\n",
        "    - The embedded vectors pass through the bidirectional LSTM encoder.\n",
        "    - The hidden states from both directions of the encoder are concatenated.\n",
        "    - The decoder generates the output sequence one token at a time, either using teacher forcing or the predicted token from the previous step.\n",
        "    \n",
        "- **Output**:\n",
        "    - The model returns the predicted sequence of tokens for the target summary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCIhYCOdY_6E"
      },
      "outputs": [],
      "source": [
        "# Define the BiLSTM model for text summarization\n",
        "class BiLSTMSummarizer(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(BiLSTMSummarizer, self).__init__()\n",
        "        # Embedding layer to convert input words to word embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM encoder with bidirectionality to capture context from both directions\n",
        "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "\n",
        "        # Decoder with LSTM, input is the output from the encoder (concatenated hidden states)\n",
        "        self.decoder = nn.LSTM(embedding_dim, hidden_dim * 2, batch_first=True)\n",
        "\n",
        "        # Fully connected layer to map the hidden states to the vocabulary size (output)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    # Forward pass through the model\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[0]  # Get batch size\n",
        "        trg_len = trg.shape[1]  # Get the length of the target sequence\n",
        "        trg_vocab_size = self.fc.out_features  # Get the output vocabulary size\n",
        "\n",
        "        # Initialize the output tensor with zeros (batch_size, trg_len, vocab_size)\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(src.device)\n",
        "\n",
        "        # Pass the source sentence through the embedding layer\n",
        "        embedded = self.embedding(src)\n",
        "\n",
        "        # Pass the embeddings through the bidirectional LSTM encoder\n",
        "        enc_output, (hidden, cell) = self.encoder(embedded)\n",
        "\n",
        "        # Combine the hidden states from both directions (concatenate)\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1).unsqueeze(0)\n",
        "        cell = torch.cat((cell[-2,:,:], cell[-1,:,:]), dim=1).unsqueeze(0)\n",
        "\n",
        "        # Start decoding with the first token (usually <sos>)\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        # Loop over each time step in the target sequence\n",
        "        for t in range(1, trg_len):\n",
        "            input_embedded = self.embedding(input).unsqueeze(1)  # Embed the current input token\n",
        "            output, (hidden, cell) = self.decoder(input_embedded, (hidden, cell))  # Decode one step\n",
        "            prediction = self.fc(output.squeeze(1))  # Pass decoder output through fully connected layer\n",
        "\n",
        "            outputs[:, t] = prediction  # Store the prediction at current time step\n",
        "\n",
        "            # Use teacher forcing (feeding correct output token back into the model)\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = prediction.argmax(1)  # Get the predicted token\n",
        "            input = trg[:, t] if teacher_force else top1  # Decide whether to use teacher forcing or not\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCuzLFlBAgT9"
      },
      "source": [
        "### Dataset Class for Text Summarization\n",
        "\n",
        "The `SummarizationDataset` class is a custom PyTorch `Dataset` that prepares data (articles and their corresponding summaries) for the text summarization task. Here's a breakdown of its functionality:\n",
        "\n",
        "1. **Initialization (`__init__`)**:\n",
        "    - **Parameters**:\n",
        "        - `articles`: A list containing the source texts (input articles).\n",
        "        - `summaries`: A list containing the target texts (summaries).\n",
        "        - `vocab`: A dictionary mapping words to their corresponding indices (vocabulary).\n",
        "        - `max_length`: The maximum length for input and output sequences, used for padding or truncating.\n",
        "\n",
        "    - **Purpose**:\n",
        "        - It initializes the dataset with articles, summaries, and the vocabulary while setting a maximum sequence length for consistent data processing.\n",
        "\n",
        "2. **Dataset Length (`__len__`)**:\n",
        "    - **Purpose**:\n",
        "        - This method returns the number of samples in the dataset by returning the length of the `articles` list.\n",
        "\n",
        "3. **Fetching Data Sample (`__getitem__`)**:\n",
        "    - **Purpose**:\n",
        "        - This method retrieves one sample (a pair of an article and its summary) from the dataset, converts the text into a sequence of token indices, and ensures the sequence length is consistent by padding or truncating the sequences.\n",
        "    \n",
        "    - **Process**:\n",
        "        - Each article and summary is tokenized by converting the text into a list of indices based on the provided vocabulary (`vocab`).\n",
        "        - Special tokens like `<sos>` (start of sentence) and `<eos>` (end of sentence) are added at the beginning and end of each sequence.\n",
        "        - If any words are not found in the vocabulary, they are replaced with the `<unk>` (unknown) token.\n",
        "        - Padding (`<pad>`) is added to sequences that are shorter than the maximum length (`max_length`), ensuring all sequences are of the same length for batch processing.\n",
        "\n",
        "    - **Output**:\n",
        "        - The method returns two tensors: one for the article and one for the corresponding summary, both padded or truncated to the same length (`max_length`).\n",
        "\n",
        "#### Special Tokens:\n",
        "- **`<sos>`**: Marks the start of a sentence.\n",
        "- **`<eos>`**: Marks the end of a sentence.\n",
        "- **`<pad>`**: Used to pad sequences to ensure consistent length across the dataset.\n",
        "- **`<unk>`**: Represents unknown or out-of-vocabulary words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBEvTN3KY_6F"
      },
      "outputs": [],
      "source": [
        "# Dataset class to prepare data for summarization\n",
        "class SummarizationDataset(Dataset):\n",
        "    def __init__(self, articles, summaries, vocab, max_length=100):\n",
        "        self.articles = articles  # List of articles (source text)\n",
        "        self.summaries = summaries  # List of summaries (target text)\n",
        "        self.vocab = vocab  # Vocabulary mapping\n",
        "        self.max_length = max_length  # Maximum sequence length for padding/truncating\n",
        "\n",
        "    # Return the length of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.articles)\n",
        "\n",
        "    # Return a sample of data (article, summary) as tensors\n",
        "    def __getitem__(self, idx):\n",
        "        article = self.articles[idx]\n",
        "        summary = self.summaries[idx]\n",
        "\n",
        "        # Convert article to a list of token indices\n",
        "        article_indices = [self.vocab['<sos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in article][:self.max_length-2] + [self.vocab['<eos>']]\n",
        "        summary_indices = [self.vocab['<sos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in summary][:self.max_length-2] + [self.vocab['<eos>']]\n",
        "\n",
        "        # Pad sequences to max_length\n",
        "        article_indices = article_indices + [self.vocab['<pad>']] * (self.max_length - len(article_indices))\n",
        "        summary_indices = summary_indices + [self.vocab['<pad>']] * (self.max_length - len(summary_indices))\n",
        "\n",
        "        return torch.tensor(article_indices), torch.tensor(summary_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZtwymucAgT-"
      },
      "source": [
        "### Data Loading and Preprocessing\n",
        "\n",
        "1. **Load Data (`load_data`)**:\n",
        "    - Loads headlines and contents from a CSV file.\n",
        "    - **Input**: CSV file path.\n",
        "    - **Output**: Lists of headlines and content.\n",
        "    \n",
        "2. **Tokenize Text (`tokenize`)**:\n",
        "    - Tokenizes and lowercases the text using NLTK.\n",
        "    - **Input**: Text string.\n",
        "    - **Output**: Tokenized words.\n",
        "\n",
        "3. **Build Vocabulary (`build_vocab`)**:\n",
        "    - Creates a word-to-index vocabulary based on word frequency.\n",
        "    - **Input**: Tokenized texts, minimum frequency (`min_freq=2`).\n",
        "    - **Output**: `word2idx` and `idx2word` mappings, including special tokens (`<pad>`, `<unk>`, `<sos>`, `<eos>`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgn_dCLbY_6H"
      },
      "outputs": [],
      "source": [
        "# Load the dataset from a CSV file (replace with your actual file path and column names)\n",
        "file_path = r\"D:\\New folder\\CODDING STUFF\\Sem 5\\NLPL\\Assignment 4\\Hindi Summarization-20241003\\hindi_news_dataset.csv\"\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df['Headline'].tolist(), df['Content'].tolist()  # Replace 'Headline' and 'Content' with actual column names\n",
        "\n",
        "# Tokenize text using word_tokenize from nltk\n",
        "def tokenize(text):\n",
        "    return word_tokenize(text.lower())  # Tokenize and lowercase the text\n",
        "\n",
        "# Build vocabulary from the dataset\n",
        "def build_vocab(texts, min_freq=2):\n",
        "    word_freq = Counter()  # Count word frequencies\n",
        "    for text in texts:\n",
        "        word_freq.update(text)\n",
        "\n",
        "    # Initialize special tokens\n",
        "    vocab = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n",
        "\n",
        "    # Add words with frequency >= min_freq\n",
        "    for word, freq in word_freq.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[word] = len(vocab)\n",
        "\n",
        "    return vocab, {v: k for k, v in vocab.items()}  # Return word2idx and idx2word mappings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8194OwXAgT-"
      },
      "source": [
        "### Data Preparation for Text Summarization\n",
        "\n",
        "1. **Load and Tokenize Data**:\n",
        "    - The articles and summaries are loaded from the CSV file and tokenized into word tokens.\n",
        "    - **Steps**:\n",
        "        - `articles, summaries = load_data(file_path)` loads the data.\n",
        "        - `tokenized_articles = [tokenize(article) for article in articles]` tokenizes each article.\n",
        "        - `tokenized_summaries = [tokenize(summary) for summary in summaries]` tokenizes each summary.\n",
        "\n",
        "2. **Build Vocabulary**:\n",
        "    - The vocabulary is built from the tokenized articles and summaries.\n",
        "    - **Step**:\n",
        "        - `vocab, inv_vocab = build_vocab(tokenized_articles + tokenized_summaries)` creates word-to-index and index-to-word mappings.\n",
        "\n",
        "3. **Split Data**:\n",
        "    - The tokenized data is split into training, validation, and test sets.\n",
        "    - **Steps**:\n",
        "        - `train_test_split` is used to split the data, first into training and test sets (80-20 split), then further splitting the training set to create a validation set (10% of training).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHfREZHwY_6I"
      },
      "outputs": [],
      "source": [
        "# Load and tokenize the articles and summaries\n",
        "articles, summaries = load_data(file_path)\n",
        "tokenized_articles = [tokenize(article) for article in articles]\n",
        "tokenized_summaries = [tokenize(summary) for summary in summaries]\n",
        "\n",
        "# Build vocabulary\n",
        "vocab, inv_vocab = build_vocab(tokenized_articles + tokenized_summaries)\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "train_articles, test_articles, train_summaries, test_summaries = train_test_split(tokenized_articles, tokenized_summaries, test_size=0.2, random_state=42)\n",
        "train_articles, val_articles, train_summaries, val_summaries = train_test_split(train_articles, train_summaries, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP18TCZfY_6J"
      },
      "outputs": [],
      "source": [
        "# Create datasets using the tokenized data and vocab\n",
        "train_dataset = SummarizationDataset(train_articles, train_summaries, vocab, max_length=50)\n",
        "val_dataset = SummarizationDataset(val_articles, val_summaries, vocab, max_length=50)\n",
        "test_dataset = SummarizationDataset(test_articles, test_summaries, vocab, max_length=50)\n",
        "\n",
        "# Create data loaders to feed data in batches\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X7pLGHsY_6J"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize model and hyperparameters\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available\n",
        "\n",
        "vocab_size = len(vocab)   # Size of the vocabulary\n",
        "embedding_dim = 300       # Size of word embeddings\n",
        "hidden_dim = 512          # Size of LSTM hidden state\n",
        "output_dim = vocab_size   # Output size, generally the size of the vocabulary\n",
        "\n",
        "# Initialize the BiLSTM model and move it to the device (GPU/CPU)\n",
        "model = BiLSTMSummarizer(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT2l2LK0AgT_"
      },
      "source": [
        "### Training Function\n",
        "\n",
        "The `train` function performs the training process, updating model weights using batches of data.\n",
        "\n",
        "1. **Training Mode**:\n",
        "    - `model.train()` enables training-specific behaviors (like dropout).\n",
        "\n",
        "2. **Batch Processing**:\n",
        "    - Each batch (`src`, `trg`) is passed through the model, and outputs are reshaped for loss calculation.\n",
        "\n",
        "3. **Loss and Backpropagation**:\n",
        "    - Loss is calculated between predicted and target sequences, followed by backpropagation.\n",
        "\n",
        "4. **Gradient Clipping**:\n",
        "    - Gradients are clipped to prevent explosion (`clip=1`).\n",
        "\n",
        "5. **Optimizer Step**:\n",
        "    - Weights are updated using the optimizer.\n",
        "\n",
        "6. **Return**:\n",
        "    - Average loss per epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZGJGt5-Y_6K"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train(model, iterator, optimizer, criterion, device, clip=1, teacher_forcing_ratio=0.5):\n",
        "    model.train()  # Set model to training mode\n",
        "    epoch_loss = 0\n",
        "    for batch in tqdm(iterator, desc=\"Training\"):  # Iterate over batches\n",
        "        src, trg = batch\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        output = model(src, trg, teacher_forcing_ratio)  # Forward pass\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].reshape(-1, output_dim)  # Reshape output for loss calculation\n",
        "        trg = trg[:, 1:].reshape(-1)  # Flatten target sequence\n",
        "\n",
        "        loss = criterion(output, trg)  # Calculate loss\n",
        "        loss.backward()  # Backpropagate\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  # Clip gradients to avoid exploding gradient\n",
        "\n",
        "        optimizer.step()  # Update parameters\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkVaqJpgAgT_"
      },
      "source": [
        "### Evaluation Function\n",
        "\n",
        "The `evaluate` function assesses the model's performance on validation or test data, without updating the model's weights.\n",
        "\n",
        "1. **Evaluation Mode**:\n",
        "    - `model.eval()` disables training-specific behaviors like dropout.\n",
        "  \n",
        "2. **No Gradient Calculation**:\n",
        "    - `torch.no_grad()` ensures no gradients are computed, saving memory and speeding up the evaluation.\n",
        "\n",
        "3. **Batch Processing**:\n",
        "    - For each batch, the model is run with teacher forcing disabled (`teacher_forcing_ratio=0`).\n",
        "    - The output and target are reshaped to match dimensions for loss calculation.\n",
        "\n",
        "4. **Loss Calculation**:\n",
        "    - Loss is computed between model predictions and target sequences.\n",
        "\n",
        "5. **Return**:\n",
        "    - The average loss over all batches is returned.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUtmmZLIY_6L"
      },
      "outputs": [],
      "source": [
        "# Evaluation function\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for batch in tqdm(iterator, desc=\"Evaluating\"):\n",
        "            src, trg = batch\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            output = model(src, trg, 0)  # Turn off teacher forcing during evaluation\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)  # Reshape output for loss calculation\n",
        "            trg = trg[:, 1:].reshape(-1)  # Flatten target sequence\n",
        "\n",
        "            loss = criterion(output, trg)  # Calculate loss\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH6H2fzGAgT_"
      },
      "source": [
        "### Beam Search for Text Generation\n",
        "\n",
        "The `beam_search` function implements the beam search algorithm for sequence generation. This approach is used to generate the best possible output sequence by exploring multiple hypotheses at each decoding step.\n",
        "\n",
        "1. **Embedding and Encoding**:\n",
        "    - The input sequence is first embedded using the model's embedding layer.\n",
        "    - The embedded sequence is passed through the encoder (a bi-directional LSTM), and the final hidden and cell states are obtained.\n",
        "\n",
        "2. **Beam Initialization**:\n",
        "    - The beam is initialized with the start-of-sequence token (`<sos>`), a score of 0, and the hidden and cell states from the encoder.\n",
        "\n",
        "3. **Beam Search Process**:\n",
        "    - For each time step, each sequence in the current beam is extended by predicting the next token using the decoder.\n",
        "    - The top `beam_width` predictions (tokens with the highest probabilities) are selected.\n",
        "    - These new sequences are added to the beam, and the beam is updated with the top `beam_width` sequences based on their cumulative scores.\n",
        "\n",
        "4. **Handling Sequence Completion**:\n",
        "    - If a sequence reaches the end-of-sequence token (`<eos>`) and is of sufficient length, it is added to the list of complete hypotheses.\n",
        "\n",
        "5. **Final Sequence Selection**:\n",
        "    - Once the beam search is complete or the maximum length is reached, the best sequence is selected from the completed hypotheses.\n",
        "    - If no sequence ends with `<eos>`, the best incomplete sequence is chosen.\n",
        "\n",
        "6. **Output**:\n",
        "    - The selected sequence of token indices is converted back to words using the `inv_vocab` (index-to-word) mapping.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFZaxsetY_6L"
      },
      "outputs": [],
      "source": [
        "def beam_search(model, src, vocab, inv_vocab, beam_width=3, max_length=50, min_length=10, device='gpu'):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Embedding the input sequence\n",
        "        embedded = model.embedding(src)  # shape: (batch_size, seq_len, embedding_dim)\n",
        "        enc_output, (hidden, cell) = model.encoder(embedded)  # LSTM encoder output\n",
        "\n",
        "        # In case of bi-directional LSTM, combine the hidden states\n",
        "        if model.encoder.bidirectional:\n",
        "            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)  # shape: (batch_size, hidden_dim)\n",
        "            cell = torch.cat((cell[-2, :, :], cell[-1, :, :]), dim=1)        # shape: (batch_size, hidden_dim)\n",
        "        else:\n",
        "            hidden = hidden[-1, :, :]  # Take the last layer if not bi-directional\n",
        "            cell = cell[-1, :, :]      # Take the last layer if not bi-directional\n",
        "\n",
        "        # Now we process one sequence at a time, so set batch size to 1\n",
        "        hidden = hidden.unsqueeze(0)  # shape: (1, batch_size, hidden_dim)\n",
        "        cell = cell.unsqueeze(0)      # shape: (1, batch_size, hidden_dim)\n",
        "\n",
        "        # Initialize the beam with the start-of-sequence token\n",
        "        beam = [([vocab['<sos>']], 0, hidden[:, 0:1, :], cell[:, 0:1, :])]  # Start with one sequence\n",
        "        complete_hypotheses = []\n",
        "\n",
        "        # Perform beam search\n",
        "        for t in range(max_length):\n",
        "            new_beam = []\n",
        "            for seq, score, hidden, cell in beam:\n",
        "                # If end-of-sequence token is reached and length is >= min_length, add to complete hypotheses\n",
        "                if seq[-1] == vocab['<eos>'] and len(seq) >= min_length:\n",
        "                    complete_hypotheses.append((seq, score))\n",
        "                    continue\n",
        "\n",
        "                # Prepare the input for the decoder (last predicted token)\n",
        "                input = torch.LongTensor([seq[-1]]).unsqueeze(0).to(device)  # shape: (1, 1)\n",
        "                input_embedded = model.embedding(input)  # shape: (1, 1, embedding_dim)\n",
        "\n",
        "                # Pass through the decoder with the current hidden and cell states\n",
        "                output, (hidden, cell) = model.decoder(input_embedded, (hidden, cell))  # hidden, cell are (1, 1, hidden_dim)\n",
        "                predictions = model.fc(output.squeeze(1))  # shape: (1, vocab_size)\n",
        "\n",
        "                # Prevent EOS if sequence is shorter than minimum length\n",
        "                if len(seq) < min_length:\n",
        "                    predictions[0][vocab['<eos>']] = float('-inf')\n",
        "\n",
        "                # Get top beam_width predictions\n",
        "                top_preds = torch.topk(predictions, beam_width, dim=1)\n",
        "\n",
        "                # For each top prediction, extend the sequence and update the beam\n",
        "                for i in range(beam_width):\n",
        "                    new_seq = seq + [top_preds.indices[0][i].item()]\n",
        "                    new_score = score - top_preds.values[0][i].item()  # Negative log probability\n",
        "                    new_hidden = hidden.clone()\n",
        "                    new_cell = cell.clone()\n",
        "                    new_beam.append((new_seq, new_score, new_hidden, new_cell))\n",
        "\n",
        "            # Sort by score and keep top beam_width sequences\n",
        "            beam = sorted(new_beam, key=lambda x: x[1])[:beam_width]\n",
        "\n",
        "            if len(complete_hypotheses) >= beam_width:\n",
        "                break\n",
        "\n",
        "        # Sort and return the best sequence\n",
        "        complete_hypotheses = sorted(complete_hypotheses, key=lambda x: x[1])\n",
        "        if complete_hypotheses:\n",
        "            best_seq = complete_hypotheses[0][0]\n",
        "        else:\n",
        "            best_seq = beam[0][0]\n",
        "\n",
        "    # Convert sequence of indices back to words\n",
        "    return [inv_vocab[idx] for idx in best_seq if idx not in [vocab['<sos>'], vocab['<eos>'], vocab['<pad>']]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYMa8KYbY_6M"
      },
      "outputs": [],
      "source": [
        "# Save model function\n",
        "def save_model(model, vocab, filepath):\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'vocab': vocab\n",
        "    }, filepath)\n",
        "    print(f\"Model saved to {'Copy_of_Hindi_Summarization_Beam_Search copy.ipynb'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCI_lkWhY_6M"
      },
      "outputs": [],
      "source": [
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyyssAHKY_6M",
        "outputId": "0b91ef58-6819-4a38-d15d-bceaade57b1f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [37:08<00:00,  1.07s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:59<00:00,  3.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 5.151\n",
            "\t Val. Loss: 5.922\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [37:06<00:00,  1.07s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [01:01<00:00,  3.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 02\n",
            "\tTrain Loss: 3.127\n",
            "\t Val. Loss: 4.733\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:12<00:00,  1.04s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:57<00:00,  4.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 03\n",
            "\tTrain Loss: 2.224\n",
            "\t Val. Loss: 4.046\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:19<00:00,  1.04s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:57<00:00,  4.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 04\n",
            "\tTrain Loss: 1.744\n",
            "\t Val. Loss: 3.587\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [37:09<00:00,  1.07s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [01:01<00:00,  3.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 05\n",
            "\tTrain Loss: 1.435\n",
            "\t Val. Loss: 3.243\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [37:09<00:00,  1.07s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [01:02<00:00,  3.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 06\n",
            "\tTrain Loss: 1.211\n",
            "\t Val. Loss: 3.036\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:58<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:58<00:00,  3.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 07\n",
            "\tTrain Loss: 1.060\n",
            "\t Val. Loss: 2.881\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:51<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [01:00<00:00,  3.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 08\n",
            "\tTrain Loss: 0.934\n",
            "\t Val. Loss: 2.765\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:47<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:59<00:00,  3.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 09\n",
            "\tTrain Loss: 0.845\n",
            "\t Val. Loss: 2.638\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:51<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:57<00:00,  4.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10\n",
            "\tTrain Loss: 0.769\n",
            "\t Val. Loss: 2.568\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\t Val. Loss: {val_loss:.3f}')\n",
        "\n",
        "    # Save model if validation loss improves\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        save_model(model, vocab, 'best_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Di3pobeY_6N"
      },
      "outputs": [],
      "source": [
        "# Load model function\n",
        "def load_model(filepath, device):\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "    vocab = checkpoint['vocab']\n",
        "    model = BiLSTMSummarizer(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model, checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPyf-nlSY_6N",
        "outputId": "4bb48d10-785b-4019-c8b5-dddc650e6a43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\deven\\AppData\\Local\\Temp\\ipykernel_3816\\114305855.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filepath, map_location=device)\n",
            "Evaluating: 100%|██████████| 580/580 [02:25<00:00,  4.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 2.572\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating summaries: 100%|██████████| 580/580 [03:03<00:00,  3.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE scores:\n",
            "{'rouge-1': {'r': 0.8083014297750529, 'p': 0.8337258642152, 'f': 0.8193316370463417}, 'rouge-2': {'r': 0.728746530253046, 'p': 0.7376144754509335, 'f': 0.7327716971193842}, 'rouge-l': {'r': 0.786234929785507, 'p': 0.8089017092229435, 'f': 0.7960673826807474}}\n"
          ]
        }
      ],
      "source": [
        "# Load the best model for testing\n",
        "best_model, _ = load_model('best_model.pth', device)\n",
        "\n",
        "# Test the model\n",
        "test_loss = evaluate(best_model, test_loader, criterion, device)\n",
        "print(f'Test Loss: {test_loss:.3f}')\n",
        "\n",
        "# Evaluate using ROUGE score\n",
        "rouge = Rouge()\n",
        "best_model.eval()\n",
        "predictions = []\n",
        "references = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Generating summaries\"):\n",
        "        src, trg = batch\n",
        "        src = src.to(device)\n",
        "        pred = beam_search(best_model, src, vocab, inv_vocab, min_length=10, device=device)  # Set minimum length\n",
        "        predictions.extend([' '.join(pred)])\n",
        "        references.extend([' '.join([inv_vocab[idx.item()] for idx in trg[0] if idx.item() not in [vocab['<sos>'], vocab['<eos>'], vocab['<pad>']]])])\n",
        "\n",
        "# Ensure all predictions meet the minimum length\n",
        "min_length = 10  # Set this to your desired minimum length\n",
        "predictions = [p if len(p.split()) >= min_length else p + ' ' + ' '.join(['<pad>'] * (min_length - len(p.split()))) for p in predictions]\n",
        "\n",
        "scores = rouge.get_scores(predictions, references, avg=True)\n",
        "print(\"ROUGE scores:\")\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y68PKEeYY_6N",
        "outputId": "92d0777f-cd84-4b63-efb5-8a0e8a4b2447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading pre-trained model...\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading pre-trained model...\")\n",
        "trained_model, checkpoint = load_model('best_model.pth', device)\n",
        "vocab = checkpoint['vocab']\n",
        "inv_vocab = {v: k for k, v in vocab.items()}\n",
        "trained_model = trained_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMv-l-VmY_6O"
      },
      "outputs": [],
      "source": [
        "# Modified Summarization bot\n",
        "def summarize_text(model, vocab, inv_vocab, text, max_length=100, min_length=10, beam_width=3, device='cpu', debug=False):\n",
        "    model.eval()\n",
        "    tokens = tokenize(text)[:max_length]\n",
        "    indices = [vocab['<sos>']] + [vocab.get(token, vocab['<unk>']) for token in tokens] + [vocab['<eos>']]\n",
        "    src = torch.LongTensor(indices).unsqueeze(0).to(device)\n",
        "\n",
        "    summary = beam_search(model, src, vocab, inv_vocab, beam_width, max_length, min_length, device)\n",
        "\n",
        "    if debug:\n",
        "        print(\"Input tokens:\", tokens)\n",
        "        print(\"Input indices:\", indices)\n",
        "        print(\"Generated indices:\", [vocab[word] for word in summary])\n",
        "        print(\"Summary length:\", len(summary))\n",
        "\n",
        "    return ' '.join(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6HLvvrkY_6O",
        "outputId": "b4210c5f-ccef-496b-9339-27f7b8ec863a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input tokens: ['ऑस्ट्रेलिया', 'ने', 'ब्लूमफोनटीन', 'में', 'पहले', 'वनडे', 'में', 'दक्षिण', 'अफ्रीका', 'को', '3-विकेट', 'से', 'हरा', 'दिया।', 'यह', '12', 'वर्षों', 'में', 'दक्षिण', 'अफ्रीका', 'के', 'खिलाफ', 'उसकी', 'धरती', 'पर', 'ऑस्ट्रेलिया', 'की', 'पहली', 'वनडे', 'जीत', 'है।', 'ऑस्ट्रेलिया', 'का', 'स्कोर', '16.3', 'ओवर', 'में', '113/7', 'था', 'लेकिन', 'मार्नस', 'लबुशेन', 'और', 'ऐश्टन', 'एगर', 'की', '112', '*', 'रनों', 'की', 'साझेदारी', 'की', 'बदौलत', 'उसने', '40.2', 'ओवर', 'में', 'लक्ष्य', 'हासिल', 'कर', 'लिया।']\n",
            "Input indices: [2, 3351, 83, 29389, 10, 1276, 3352, 10, 3184, 965, 76, 29390, 37, 3192, 27649, 229, 605, 489, 10, 3184, 965, 12, 323, 431, 3771, 98, 3351, 8, 575, 3352, 2706, 27646, 3351, 24, 3490, 29391, 3396, 10, 29392, 28, 2458, 3769, 3770, 73, 29393, 29394, 8, 10147, 3628, 8210, 8, 11848, 8, 3884, 4727, 29395, 3396, 10, 1983, 3806, 103, 27891, 3]\n",
            "Generated indices: [3352, 490, 3358, 1958, 10, 1276, 1243, 10, 3351, 12, 323, 1243, 10, 5887, 37, 6826, 496, 59, 3728, 3701, 29189, 76, 3578, 1862, 27649, 3351, 83, 1032, 852, 3351, 10, 3351, 8, 3434, 10, 3408, 3556, 83, 8519, 3396, 10, 3351, 76, 3192, 478, 27647, 3351, 83, 83, 1032, 852, 87, 8, 83, 83, 211, 852, 3351, 8]\n",
            "Summary length: 59\n",
            "Generated Summary:\n",
            "वनडे विश्व कप 2023 में पहले मैच में ऑस्ट्रेलिया के खिलाफ मैच में गलती से बचने करते हुए ओपनर फखर ज़मान को आउट करार दिया। ऑस्ट्रेलिया ने बताया कि ऑस्ट्रेलिया में ऑस्ट्रेलिया की पारी में पाकिस्तानी बल्लेबाज़ों ने उस ओवर में ऑस्ट्रेलिया को हरा दिया था। ऑस्ट्रेलिया ने ने बताया कि भारत की ने ने कहा कि ऑस्ट्रेलिया की\n",
            "Summary length: 59\n"
          ]
        }
      ],
      "source": [
        "# Example usage of the summarization bot\n",
        "input_text = \"ऑस्ट्रेलिया ने ब्लूमफोनटीन में पहले वनडे में दक्षिण अफ्रीका को 3-विकेट से हरा दिया। यह 12 वर्षों में दक्षिण अफ्रीका के खिलाफ उसकी धरती पर ऑस्ट्रेलिया की पहली वनडे जीत है। ऑस्ट्रेलिया का स्कोर 16.3 ओवर में 113/7 था लेकिन मार्नस लबुशेन और ऐश्टन एगर की 112* रनों की साझेदारी की बदौलत उसने 40.2 ओवर में लक्ष्य हासिल कर लिया।\"\n",
        "summary = summarize_text(trained_model, vocab, inv_vocab, input_text, min_length=10, device=device, debug=True)\n",
        "print(\"Generated Summary:\")\n",
        "print(summary)\n",
        "print(\"Summary length:\", len(summary.split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1MObqUOAgUB"
      },
      "source": [
        "### ROUGE Score Evaluation\n",
        "\n",
        "The following ROUGE scores evaluate the performance of the text summarization model in terms of **ROUGE-1**, **ROUGE-2**, and **ROUGE-L**.\n",
        "\n",
        "---\n",
        "\n",
        "#### ROUGE-1 (Unigrams)\n",
        "- **Recall**: 0.808\n",
        "- **Precision**: 0.834\n",
        "- **F1-score**: 0.819\n",
        "\n",
        "**Analysis**:  \n",
        "ROUGE-1 measures the overlap of unigrams (individual words) between the generated summaries and reference summaries. The recall score of 0.808 means that the model captures 80.8% of relevant unigrams from the reference summaries. The precision score of 0.834 shows that 83.4% of the unigrams generated by the model are correct. The F1-score, which balances precision and recall, is 0.819, indicating strong overall performance in capturing individual words.\n",
        "\n",
        "---\n",
        "\n",
        "#### ROUGE-2 (Bigrams)\n",
        "- **Recall**: 0.729\n",
        "- **Precision**: 0.738\n",
        "- **F1-score**: 0.733\n",
        "\n",
        "**Analysis**:  \n",
        "ROUGE-2 focuses on the overlap of bigrams (pairs of consecutive words). The recall of 0.729 indicates that 72.9% of the relevant bigrams from the reference summaries are captured by the model. The precision score of 0.738 means that 73.8% of the bigrams in the generated summaries are correct. The F1-score of 0.733 reflects the model's reasonable performance in capturing longer word sequences, though it is lower than ROUGE-1, as expected, due to the increased complexity of matching bigrams.\n",
        "\n",
        "---\n",
        "\n",
        "#### ROUGE-L (Longest Common Subsequence)\n",
        "- **Recall**: 0.786\n",
        "- **Precision**: 0.809\n",
        "- **F1-score**: 0.796\n",
        "\n",
        "**Analysis**:  \n",
        "ROUGE-L evaluates the longest common subsequence between the generated and reference summaries, focusing on capturing the overall structure of the text. The recall score of 0.786 shows that the model aligns well with the reference summaries, capturing 78.6% of the longest subsequences. Precision is higher at 0.809, meaning that 80.9% of the generated subsequences are correct. The F1-score of 0.796 demonstrates strong performance in maintaining the structural integrity of the summaries, comparable to ROUGE-1.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary of Comparison:\n",
        "\n",
        "- **ROUGE-1** has the highest scores, reflecting the model's strength in capturing individual words accurately.\n",
        "- **ROUGE-2** shows lower scores, as expected, indicating that the model finds it more challenging to capture exact bigram (two-word sequence) matches.\n",
        "- **ROUGE-L** closely follows ROUGE-1 in performance, highlighting the model’s ability to capture the overall sequence structure and flow of the summaries.\n",
        "\n",
        "Overall, the model performs well in generating summaries with strong word overlap (ROUGE-1) and sequence structure (ROUGE-L), though it shows some difficulty in matching consecutive word pairs (ROUGE-2).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install texlive texlive-xetex texlive-latex-extra pandoc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GehIiEJZA1b9",
        "outputId": "9824c7f3-762f-46ad-d1be-9b9c5c1b652c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  dvisvgm fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono fonts-texgyre\n",
            "  fonts-urw-base35 libapache-pom-java libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3\n",
            "  libcommons-logging-java libcommons-parent-java libfontbox-java libfontenc1 libgs9 libgs9-common\n",
            "  libidn12 libijs-0.35 libjbig2dec0 libkpathsea6 libpdfbox-java libptexenc1 libruby3.0 libsynctex2\n",
            "  libteckit0 libtexlua53 libtexluajit2 libwoff1 libzzip-0-13 lmodern pandoc-data poppler-data\n",
            "  preview-latex-style rake ruby ruby-net-telnet ruby-rubygems ruby-webrick ruby-xmlrpc ruby3.0\n",
            "  rubygems-integration t1utils teckit tex-common tex-gyre texlive-base texlive-binaries\n",
            "  texlive-fonts-recommended texlive-latex-base texlive-latex-recommended texlive-pictures\n",
            "  texlive-plain-generic tipa xfonts-encodings xfonts-utils\n",
            "Suggested packages:\n",
            "  fonts-noto fonts-freefont-otf | fonts-freefont-ttf libavalon-framework-java\n",
            "  libcommons-logging-java-doc libexcalibur-logkit-java liblog4j1.2-java texlive-luatex\n",
            "  pandoc-citeproc context wkhtmltopdf librsvg2-bin groff ghc nodejs php python libjs-mathjax\n",
            "  libjs-katex citation-style-language-styles poppler-utils ghostscript fonts-japanese-mincho\n",
            "  | fonts-ipafont-mincho fonts-japanese-gothic | fonts-ipafont-gothic fonts-arphic-ukai\n",
            "  fonts-arphic-uming fonts-nanum ri ruby-dev bundler debhelper gv | postscript-viewer perl-tk xpdf\n",
            "  | pdf-viewer xzdec texlive-fonts-recommended-doc texlive-latex-base-doc python3-pygments\n",
            "  icc-profiles libfile-which-perl libspreadsheet-parseexcel-perl texlive-latex-extra-doc\n",
            "  texlive-latex-recommended-doc texlive-pstricks dot2tex prerex texlive-pictures-doc vprerex\n",
            "  default-jre-headless tipa-doc\n",
            "The following NEW packages will be installed:\n",
            "  dvisvgm fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono fonts-texgyre\n",
            "  fonts-urw-base35 libapache-pom-java libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3\n",
            "  libcommons-logging-java libcommons-parent-java libfontbox-java libfontenc1 libgs9 libgs9-common\n",
            "  libidn12 libijs-0.35 libjbig2dec0 libkpathsea6 libpdfbox-java libptexenc1 libruby3.0 libsynctex2\n",
            "  libteckit0 libtexlua53 libtexluajit2 libwoff1 libzzip-0-13 lmodern pandoc pandoc-data\n",
            "  poppler-data preview-latex-style rake ruby ruby-net-telnet ruby-rubygems ruby-webrick ruby-xmlrpc\n",
            "  ruby3.0 rubygems-integration t1utils teckit tex-common tex-gyre texlive texlive-base\n",
            "  texlive-binaries texlive-fonts-recommended texlive-latex-base texlive-latex-extra\n",
            "  texlive-latex-recommended texlive-pictures texlive-plain-generic texlive-xetex tipa\n",
            "  xfonts-encodings xfonts-utils\n",
            "0 upgraded, 59 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 202 MB of archives.\n",
            "After this operation, 728 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1build1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lato all 2.0-2.1 [2,696 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2,171 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-common all 6.17 [33.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-urw-base35 all 20200910-1 [6,367 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9-common all 9.55.0~dfsg1-0ubuntu5.9 [752 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libidn12 amd64 1.38-4ubuntu1 [60.0 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libijs-0.35 amd64 0.35-15build2 [16.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjbig2dec0 amd64 0.19-3build2 [64.7 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9 amd64 9.55.0~dfsg1-0ubuntu5.9 [5,033 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libkpathsea6 amd64 2021.20210626.59705-1ubuntu0.2 [60.4 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwoff1 amd64 1.0.2-1build4 [45.2 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 dvisvgm amd64 2.13.1-1 [1,221 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-lmodern all 2.004.5-6.1 [4,532 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-texgyre all 20180621-3.1 [10.2 MB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libapache-pom-java all 18-1 [4,720 B]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [115 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm-extensions0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [25.1 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-parent-java all 43-1 [10.8 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-logging-java all 1.2-2 [60.3 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libptexenc1 amd64 2021.20210626.59705-1ubuntu0.2 [39.1 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 rubygems-integration all 1.18 [5,336 B]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby3.0 amd64 3.0.2-7ubuntu2.7 [50.1 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-rubygems all 3.3.5-2 [228 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby amd64 1:3.0~exp1 [5,100 B]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 rake all 13.0.6-2 [61.7 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-net-telnet all 0.1.1-2 [12.6 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby-webrick all 1.7.0-3ubuntu0.1 [52.1 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby-xmlrpc all 0.3.2-1ubuntu0.1 [24.9 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libruby3.0 amd64 3.0.2-7ubuntu2.7 [5,113 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsynctex2 amd64 2021.20210626.59705-1ubuntu0.2 [55.6 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libteckit0 amd64 2.5.11+ds1-1 [421 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexlua53 amd64 2021.20210626.59705-1ubuntu0.2 [120 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexluajit2 amd64 2021.20210626.59705-1ubuntu0.2 [267 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzzip-0-13 amd64 0.13.72+dfsg.1-1.1 [27.0 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu jammy/universe amd64 lmodern all 2.004.5-6.1 [9,471 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc-data all 2.9.2.1-3ubuntu2 [81.8 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc amd64 2.9.2.1-3ubuntu2 [20.3 MB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu jammy/universe amd64 preview-latex-style all 12.2-1ubuntu1 [185 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu jammy/main amd64 t1utils amd64 1.41-4build2 [61.3 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu jammy/universe amd64 teckit amd64 2.5.11+ds1-1 [699 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-gyre all 20180621-3.1 [6,209 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 texlive-binaries amd64 2021.20210626.59705-1ubuntu0.2 [9,860 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-base all 2021.20220204-1 [21.0 MB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-fonts-recommended all 2021.20220204-1 [4,972 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-base all 2021.20220204-1 [1,128 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-recommended all 2021.20220204-1 [14.4 MB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive all 2021.20220204-1 [14.3 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfontbox-java all 1:1.8.16-2 [207 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpdfbox-java all 1:1.8.16-2 [5,199 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-pictures all 2021.20220204-1 [8,720 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-extra all 2021.20220204-1 [13.9 MB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-plain-generic all 2021.20220204-1 [27.5 MB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tipa all 2:1.3-21 [2,967 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-xetex all 2021.20220204-1 [12.4 MB]\n",
            "Fetched 202 MB in 12s (16.3 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 123629 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Selecting previously unselected package fonts-lato.\n",
            "Preparing to unpack .../01-fonts-lato_2.0-2.1_all.deb ...\n",
            "Unpacking fonts-lato (2.0-2.1) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../02-poppler-data_0.4.11-1_all.deb ...\n",
            "Unpacking poppler-data (0.4.11-1) ...\n",
            "Selecting previously unselected package tex-common.\n",
            "Preparing to unpack .../03-tex-common_6.17_all.deb ...\n",
            "Unpacking tex-common (6.17) ...\n",
            "Selecting previously unselected package fonts-urw-base35.\n",
            "Preparing to unpack .../04-fonts-urw-base35_20200910-1_all.deb ...\n",
            "Unpacking fonts-urw-base35 (20200910-1) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../05-libgs9-common_9.55.0~dfsg1-0ubuntu5.9_all.deb ...\n",
            "Unpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Selecting previously unselected package libidn12:amd64.\n",
            "Preparing to unpack .../06-libidn12_1.38-4ubuntu1_amd64.deb ...\n",
            "Unpacking libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../07-libijs-0.35_0.35-15build2_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../08-libjbig2dec0_0.19-3build2_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../09-libgs9_9.55.0~dfsg1-0ubuntu5.9_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Selecting previously unselected package libkpathsea6:amd64.\n",
            "Preparing to unpack .../10-libkpathsea6_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libwoff1:amd64.\n",
            "Preparing to unpack .../11-libwoff1_1.0.2-1build4_amd64.deb ...\n",
            "Unpacking libwoff1:amd64 (1.0.2-1build4) ...\n",
            "Selecting previously unselected package dvisvgm.\n",
            "Preparing to unpack .../12-dvisvgm_2.13.1-1_amd64.deb ...\n",
            "Unpacking dvisvgm (2.13.1-1) ...\n",
            "Selecting previously unselected package fonts-lmodern.\n",
            "Preparing to unpack .../13-fonts-lmodern_2.004.5-6.1_all.deb ...\n",
            "Unpacking fonts-lmodern (2.004.5-6.1) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../14-fonts-noto-mono_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-mono (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-texgyre.\n",
            "Preparing to unpack .../15-fonts-texgyre_20180621-3.1_all.deb ...\n",
            "Unpacking fonts-texgyre (20180621-3.1) ...\n",
            "Selecting previously unselected package libapache-pom-java.\n",
            "Preparing to unpack .../16-libapache-pom-java_18-1_all.deb ...\n",
            "Unpacking libapache-pom-java (18-1) ...\n",
            "Selecting previously unselected package libcmark-gfm0.29.0.gfm.3:amd64.\n",
            "Preparing to unpack .../17-libcmark-gfm0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package libcmark-gfm-extensions0.29.0.gfm.3:amd64.\n",
            "Preparing to unpack .../18-libcmark-gfm-extensions0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package libcommons-parent-java.\n",
            "Preparing to unpack .../19-libcommons-parent-java_43-1_all.deb ...\n",
            "Unpacking libcommons-parent-java (43-1) ...\n",
            "Selecting previously unselected package libcommons-logging-java.\n",
            "Preparing to unpack .../20-libcommons-logging-java_1.2-2_all.deb ...\n",
            "Unpacking libcommons-logging-java (1.2-2) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../21-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libptexenc1:amd64.\n",
            "Preparing to unpack .../22-libptexenc1_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package rubygems-integration.\n",
            "Preparing to unpack .../23-rubygems-integration_1.18_all.deb ...\n",
            "Unpacking rubygems-integration (1.18) ...\n",
            "Selecting previously unselected package ruby3.0.\n",
            "Preparing to unpack .../24-ruby3.0_3.0.2-7ubuntu2.7_amd64.deb ...\n",
            "Unpacking ruby3.0 (3.0.2-7ubuntu2.7) ...\n",
            "Selecting previously unselected package ruby-rubygems.\n",
            "Preparing to unpack .../25-ruby-rubygems_3.3.5-2_all.deb ...\n",
            "Unpacking ruby-rubygems (3.3.5-2) ...\n",
            "Selecting previously unselected package ruby.\n",
            "Preparing to unpack .../26-ruby_1%3a3.0~exp1_amd64.deb ...\n",
            "Unpacking ruby (1:3.0~exp1) ...\n",
            "Selecting previously unselected package rake.\n",
            "Preparing to unpack .../27-rake_13.0.6-2_all.deb ...\n",
            "Unpacking rake (13.0.6-2) ...\n",
            "Selecting previously unselected package ruby-net-telnet.\n",
            "Preparing to unpack .../28-ruby-net-telnet_0.1.1-2_all.deb ...\n",
            "Unpacking ruby-net-telnet (0.1.1-2) ...\n",
            "Selecting previously unselected package ruby-webrick.\n",
            "Preparing to unpack .../29-ruby-webrick_1.7.0-3ubuntu0.1_all.deb ...\n",
            "Unpacking ruby-webrick (1.7.0-3ubuntu0.1) ...\n",
            "Selecting previously unselected package ruby-xmlrpc.\n",
            "Preparing to unpack .../30-ruby-xmlrpc_0.3.2-1ubuntu0.1_all.deb ...\n",
            "Unpacking ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libruby3.0:amd64.\n",
            "Preparing to unpack .../31-libruby3.0_3.0.2-7ubuntu2.7_amd64.deb ...\n",
            "Unpacking libruby3.0:amd64 (3.0.2-7ubuntu2.7) ...\n",
            "Selecting previously unselected package libsynctex2:amd64.\n",
            "Preparing to unpack .../32-libsynctex2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libteckit0:amd64.\n",
            "Preparing to unpack .../33-libteckit0_2.5.11+ds1-1_amd64.deb ...\n",
            "Unpacking libteckit0:amd64 (2.5.11+ds1-1) ...\n",
            "Selecting previously unselected package libtexlua53:amd64.\n",
            "Preparing to unpack .../34-libtexlua53_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libtexluajit2:amd64.\n",
            "Preparing to unpack .../35-libtexluajit2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libzzip-0-13:amd64.\n",
            "Preparing to unpack .../36-libzzip-0-13_0.13.72+dfsg.1-1.1_amd64.deb ...\n",
            "Unpacking libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../37-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../38-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package lmodern.\n",
            "Preparing to unpack .../39-lmodern_2.004.5-6.1_all.deb ...\n",
            "Unpacking lmodern (2.004.5-6.1) ...\n",
            "Selecting previously unselected package pandoc-data.\n",
            "Preparing to unpack .../40-pandoc-data_2.9.2.1-3ubuntu2_all.deb ...\n",
            "Unpacking pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Selecting previously unselected package pandoc.\n",
            "Preparing to unpack .../41-pandoc_2.9.2.1-3ubuntu2_amd64.deb ...\n",
            "Unpacking pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Selecting previously unselected package preview-latex-style.\n",
            "Preparing to unpack .../42-preview-latex-style_12.2-1ubuntu1_all.deb ...\n",
            "Unpacking preview-latex-style (12.2-1ubuntu1) ...\n",
            "Selecting previously unselected package t1utils.\n",
            "Preparing to unpack .../43-t1utils_1.41-4build2_amd64.deb ...\n",
            "Unpacking t1utils (1.41-4build2) ...\n",
            "Selecting previously unselected package teckit.\n",
            "Preparing to unpack .../44-teckit_2.5.11+ds1-1_amd64.deb ...\n",
            "Unpacking teckit (2.5.11+ds1-1) ...\n",
            "Selecting previously unselected package tex-gyre.\n",
            "Preparing to unpack .../45-tex-gyre_20180621-3.1_all.deb ...\n",
            "Unpacking tex-gyre (20180621-3.1) ...\n",
            "Selecting previously unselected package texlive-binaries.\n",
            "Preparing to unpack .../46-texlive-binaries_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking texlive-binaries (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package texlive-base.\n",
            "Preparing to unpack .../47-texlive-base_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-base (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-fonts-recommended.\n",
            "Preparing to unpack .../48-texlive-fonts-recommended_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-fonts-recommended (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-latex-base.\n",
            "Preparing to unpack .../49-texlive-latex-base_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-base (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-latex-recommended.\n",
            "Preparing to unpack .../50-texlive-latex-recommended_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-recommended (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive.\n",
            "Preparing to unpack .../51-texlive_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive (2021.20220204-1) ...\n",
            "Selecting previously unselected package libfontbox-java.\n",
            "Preparing to unpack .../52-libfontbox-java_1%3a1.8.16-2_all.deb ...\n",
            "Unpacking libfontbox-java (1:1.8.16-2) ...\n",
            "Selecting previously unselected package libpdfbox-java.\n",
            "Preparing to unpack .../53-libpdfbox-java_1%3a1.8.16-2_all.deb ...\n",
            "Unpacking libpdfbox-java (1:1.8.16-2) ...\n",
            "Selecting previously unselected package texlive-pictures.\n",
            "Preparing to unpack .../54-texlive-pictures_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-pictures (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-latex-extra.\n",
            "Preparing to unpack .../55-texlive-latex-extra_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-extra (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-plain-generic.\n",
            "Preparing to unpack .../56-texlive-plain-generic_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-plain-generic (2021.20220204-1) ...\n",
            "Selecting previously unselected package tipa.\n",
            "Preparing to unpack .../57-tipa_2%3a1.3-21_all.deb ...\n",
            "Unpacking tipa (2:1.3-21) ...\n",
            "Selecting previously unselected package texlive-xetex.\n",
            "Preparing to unpack .../58-texlive-xetex_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-xetex (2021.20220204-1) ...\n",
            "Setting up fonts-lato (2.0-2.1) ...\n",
            "Setting up fonts-noto-mono (20201225-1build1) ...\n",
            "Setting up libwoff1:amd64 (1.0.2-1build4) ...\n",
            "Setting up libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Setting up libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libfontbox-java (1:1.8.16-2) ...\n",
            "Setting up rubygems-integration (1.18) ...\n",
            "Setting up libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\n",
            "Setting up fonts-urw-base35 (20200910-1) ...\n",
            "Setting up poppler-data (0.4.11-1) ...\n",
            "Setting up tex-common (6.17) ...\n",
            "update-language: texlive-base not installed and configured, doing nothing!\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Setting up libteckit0:amd64 (2.5.11+ds1-1) ...\n",
            "Setting up libapache-pom-java (18-1) ...\n",
            "Setting up ruby-net-telnet (0.1.1-2) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up t1utils (1.41-4build2) ...\n",
            "Setting up libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Setting up fonts-texgyre (20180621-3.1) ...\n",
            "Setting up libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up ruby-webrick (1.7.0-3ubuntu0.1) ...\n",
            "Setting up libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up fonts-lmodern (2.004.5-6.1) ...\n",
            "Setting up libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Setting up pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Setting up ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\n",
            "Setting up libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libgs9-common (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Setting up teckit (2.5.11+ds1-1) ...\n",
            "Setting up libpdfbox-java (1:1.8.16-2) ...\n",
            "Setting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Setting up preview-latex-style (12.2-1ubuntu1) ...\n",
            "Setting up libcommons-parent-java (43-1) ...\n",
            "Setting up dvisvgm (2.13.1-1) ...\n",
            "Setting up libcommons-logging-java (1.2-2) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Setting up texlive-binaries (2021.20210626.59705-1ubuntu0.2) ...\n",
            "update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n",
            "update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n",
            "Setting up lmodern (2.004.5-6.1) ...\n",
            "Setting up texlive-base (2021.20220204-1) ...\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R... \n",
            "mktexlsr: Done.\n",
            "tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n",
            "tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n",
            "tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n",
            "tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/tex-ini-files/pdftexconfig.tex\n",
            "Setting up tex-gyre (20180621-3.1) ...\n",
            "Setting up texlive-plain-generic (2021.20220204-1) ...\n",
            "Setting up texlive-latex-base (2021.20220204-1) ...\n",
            "Setting up texlive-latex-recommended (2021.20220204-1) ...\n",
            "Setting up texlive-pictures (2021.20220204-1) ...\n",
            "Setting up texlive-fonts-recommended (2021.20220204-1) ...\n",
            "Setting up tipa (2:1.3-21) ...\n",
            "Setting up texlive (2021.20220204-1) ...\n",
            "Setting up texlive-latex-extra (2021.20220204-1) ...\n",
            "Setting up texlive-xetex (2021.20220204-1) ...\n",
            "Setting up rake (13.0.6-2) ...\n",
            "Setting up libruby3.0:amd64 (3.0.2-7ubuntu2.7) ...\n",
            "Setting up ruby3.0 (3.0.2-7ubuntu2.7) ...\n",
            "Setting up ruby (1:3.0~exp1) ...\n",
            "Setting up ruby-rubygems (3.3.5-2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for tex-common (6.17) ...\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Building format(s) --all.\n",
            "\tThis may take some time... done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypandoc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geKTkf2GBESr",
        "outputId": "7b526b35-6676-451d-83e0-b1ea875a1b83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypandoc\n",
            "  Downloading pypandoc-1.14-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading pypandoc-1.14-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: pypandoc\n",
            "Successfully installed pypandoc-1.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su_AB4tnBJnE",
        "outputId": "50a51f0a-f365-401d-b26f-d9621daab825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to pdf \"/content/drive/My Drive/Colab Notebooks/Parth_tripathi_071_A3_NLP_Assignment_4 .ipynb\" --output \"Parth_tripathi_071_A3_NLP_Assignment_4\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXvqPbWHBJuA",
        "outputId": "600d3ba0-36ef-4a5a-8ccc-9ad42530278d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/drive/My Drive/Colab Notebooks/Parth_tripathi_071_A3_NLP_Assignment_4 .ipynb to pdf\n",
            "[NbConvertApp] Writing 124568 bytes to notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
            "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 113136 bytes to /content/drive/My Drive/Colab Notebooks/Parth_tripathi_071_A3_NLP_Assignment_4.pdf\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}